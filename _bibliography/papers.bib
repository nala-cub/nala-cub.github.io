---
---
@inproceedings{ebrahimi2024americasnlp,
    title = "Findings of the {A}mericas{NLP} 2024 Shared Task on Machine Translation into Indigenous Languages",
    author = "Ebrahimi, Abteen  and
      de Gibert, Ona  and
      Vazquez, Raul  and
      Coto-Solano, Rolando  and
      Denisov, Pavel  and
      Pugh, Robert  and
      Mager, Manuel  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      {von der Wense}, Katharina  and
      Rijhwani, Shruti",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
}

@inproceedings{chiruzzo2024americasnlp,
    title = "Findings of the {A}mericas{NLP} 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages",
    author = "Chiruzzo, Luis  and
      Denisov, Pavel  and
      Molina-Villegas, Alejandro  and
      Fernandez-Sabido, Silvia  and
      Coto-Solano, Rolando  and
      Ag{\"u}ero-Torales, Marvin  and
      Alvarez, Aldo  and
      Canul-Yah, Samuel  and
      Hau-Uc{\'a}n, Lorena  and
      Ebrahimi, Abteen  and
      Pugh, Robert  and
      Oncevay, Arturo  and
      Rijhwani, Shruti  and
      {von der Wense}, Katharina  and
      Mager, Manuel",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
}

@inproceedings{weber2024cogsci,
    title = "Evaluating LLMs as Tools to Support Early Vocabulary Learning",
    author = "Weber, Jennifer and Valentini, Maria and Wright, T\'{e}a and {von der Wense}, Katharina and Colunga, Eliana",
    booktitle = "Proceedings of the Annual Meeting of the Cognitive Science Society (to appear)",
    month = Juli,
    year = "2024",
}

@inproceedings{ganesh2024edm,
    title = "Prompting as Panacea? {A} Case Study of In-Context Learning Performance for Qualitative Coding of Classroom Dialog",
    author = "Ganesh, Ananya and Chandler, Chelsea and D'Mello, Sidney and Palmer, Martha and {von der Wense}, Katharina",
    booktitle = "Proceedings of the International Conference on Educational Data Mining (to appear)",
    month = Juli,
    year = "2024",
}

@inproceedings{ebrahimi2024naacl,
    title = "Zero-Shot vs. Translation-Based Cross-Lingual Transfer: {T}he Case of Lexical Gaps",
    author = "Ebrahimi, Abteen and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (to appear)",
    month = June,
    year = "2024",
}

@inproceedings{bui2024negnlp,
    title = "Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget",
    author = "Bui, Minh Duc and Schmidt, Fabian David and Glava\v{s}, Goran and {von der Wense}, Katharina",
    booktitle = "Proceedings of the Workshop on Insights from Negative Results in NLP (to appear)",
    month = June,
    year = "2024",
}

@inproceedings{bui2024trustnlp,
    title = "The Trade-off between Performance, Efficiency, and Fairness in Adapter Modules for Text Classification",
    author = "Bui, Minh Duc  and {von der Wense}, Katharina",
    booktitle = "Proceedings of the Fourth Workshop on Trustworthy Natural Language Processing (to appear)",
    month = June,
    year = "2024",
}

@inproceedings{gessler2024americasnlp,
    title = "NLP for Language Documentation: {T}wo Reasons for the Gap between Theory and Practice",
    author = "Gessler, Luke and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP) (to appear)",
    month = June,
    year = "2024",
}

@inproceedings{bui2024americasnlpst,
    title = "{JGU Mainz}'s Submission to the {AmericasNLP} 2024 Shared Task on the Creation of Educational Materials for {I}ndigenous Languages",
    author = "Bui, Minh Duc  and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP) (to appear)",
    month = June,
    year = "2024",
}

@inproceedings{wiemerslage2024eacl,
    title = "Quantifying the Hyperparameter Sensitivity of Neural Networks for Character-level Sequence-to-Sequence Tasks",
    author = "Wiemerslage, Adam and Gorman, Kyle and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics",
    month = march,
    year = "2024",
}

@inproceedings{shaier2024eacl1,
    title = "Comparing Template-based and Template-free Language Model Probing",
    author = "Shaier, Sagi and Bennett, Kevin and Hunter, Lawrence and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics",
    month = march,
    year = "2024",
}

@inproceedings{shaier2024eacl2,
    title = "Desiderata For The Context Use Of Question Answering Systems",
    author = "Shaier, Sagi and Hunter, Lawrence and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics",
    month = march,
    year = "2024",
}

@inproceedings{valentini2023emnlp,
    title = "On the Automatic Generation and Simplification of Children's Stories",
    author = "Valentini, Maria and Weber, Jennifer and Salcido, Jesus and Wright, T\'{e}a and Colunga, Eliana and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2023",
}

@inproceedings{shaier2023aacl2,
    title = "Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems",
    author = "Shaier, Sagi and Bennett, Kevin and Hunter, Lawrence and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics",
    month = november,
    year = "2023",
}

@inproceedings{shaier2023aacl1,
    title = "Who Are All The Stochastic Parrots Imitating? They Should Tell Us!",
    author = "Shaier, Sagi and Hunter, Lawrence and {von der Wense}, Katharina",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics",
    month = november,
    year = "2023",
}

@inproceedings{ganesh2023coco4mt,
    title = "Findings of the CoCo4MT 2023 Shared Task on Corpus Construction for Machine Translation",
    author = "Ganesh, Ananya and Carpuat, Marine and Chen, William and Kann, Katharina and Lignos, Constantine and Ortega, John E. and Saleva, Jonne and Tafreshi, Shabnam and Zevallos, Rodolfo",
    booktitle = "Proceedings of the Second Workshop on Corpus Generation and Corpus Augmentation for Machine Translation",
    month = september,
    year = "2023",
}

@inproceedings{mager2023americasnlp,
    title = "Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction",
    author = "Mager, Manuel and Bhatnagar, Rajat and Neubig, Graham and Vu, Ngoc Thang and Kann, Katharina",
    booktitle = "Proceedings of the Third Workshop on NLP for Indigenous Languages of the Americas",
    month = july,
    year = "2023",
}

@inproceedings{ebrahimi2023americasnlp,
    title = "Findings of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages",
    author = "Ebrahimi, Abteen and Mager, Manuel and Rijhwani, Shruti and Rice, Enora and Oncevay, Arturo and Baltazar, Claudia and Cortés, María and Montaño, Cynthia and Ortega, John E and Coto-Solano, Rolando and Cruz, Hilaria and Palmer, Alexis and Kann, Katharina",
    booktitle = "Proceedings of the Third Workshop on NLP for Indigenous Languages of the Americas",
    month = july,
    year = "2023",
}

@inproceedings{ganesh2023convai,
    title = "A Survey of Challenges and Methods in the Computational Modeling of Multi-Party Dialog",
    author = "Ganesh, Ananya and Palmer, Martha and Kann, Katharina",
    booktitle = "Proceedings of the 5th Workshop on NLP for Conversational AI",
    month = july,
    year = "2023",
}

@inproceedings{ganesh2023acl,
    title = "Mind the Gap between the Application Track and the Real World",
    author = "Ganesh, Ananya and Cao, Jie and Perkoff, Margaret and Southwell, Rosy and Palmer, Martha and Kann, Katharina",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
    month = july,
    year = "2023",
}

@inproceedings{mager2023acl,
    title = "Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers",
    author = "Mager, Manuel and Mager, Elisabeth Albine and Kann, Katharina and Vu, Ngoc Thang",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
    month = july,
    year = "2023",
}

@inproceedings{wiemerslage2023acl,
    title = "An Investigation of Noise in Morphological Inflection",
    author = "Wiemerslage, Adam and Yang, Changbing and Nicolai, Garrett and Silfverberg, Miikka and Kann, Katharina",
    booktitle = "Findings of the 61st Annual Meeting of the Association for Computational Linguistics",
    month = july,
    year = "2023",
}

@inproceedings{jie2023umap,
    title = "A Comparative Analysis of Automatic Speech Recognition Errors in Small Group Classroom Discourse",
    author = "Cao, Jie and Ganesh, Ananya and Cai, Jon and Southwell, Rosy and Perkoff, Margaret and Reagan, Michael and Kann, Katharina and Martin, James and Palmer, Martha and D'Mello, Sidney",
    booktitle = "Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization",
    month = june,
    year = "2023",
}

@inproceedings{ganesh2023aied,
    title = "Navigating Wanderland: Highlighting Off-Task Discussions in Classrooms",
    author = "Ganesh, Ananya and Chang, Michael and Dickler, Rachel and Regan, Michael and Cai, Jon and Wright-Bettner, Kristin and Pustejovsky, James and Martin, James and Flanigan, Jeff and Palmer, Martha and Kann, Katharina",
    booktitle = "Proceedings of the 24th International Conference on Artificial Intelligence in Education",
    month = july,
    year = "2023",
}

@inproceedings{ebrahimieacl2023,
    title = "Meeting the Needs of Low-Resource Languages: Exploring Automatic Alignments via Pretrained Models",
    author = "Ebrahimi, Abteen and McCarthy, Arya D. and Oncevay, Arturo and Ortega, John E. and Chiruzzo, Luis and Coto-Solano, Rolando and Giménez-Lugo, Gustavo A. and Kann, Katharina",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
}

@InProceedings{pmlr-v220-ebrahimi22a,
  title =      {Findings of the Second AmericasNLP Competition on Speech-to-Text Translation},
  author =       {Ebrahimi, Abteen and Mager, Manuel and Wiemerslage, Adam and Denisov, Pavel and Oncevay, Arturo and Liu, Danni and Koneru, Sai and Ugan, Enes Yavuz and Li, Zhaolin and Niehues, Jan and Romero, Monica and Torre, Ivan G and Alum\"{a}e, Tanel and Kong, Jiaming and Polezhaev, Sergey and Belousov, Yury and Chen, Wei-Rui and Sullivan, Peter and Adebara, Ife and Talafha, Bashar and Inciarte, Alcides Alcoba and Abdul-Mageed, Muhammad and Chiruzzo, Luis and Coto-Solano, Rolando and Cruz, Hilaria and Flores-Sol\'{o}rzano, Sof\'{i}a and L\'{o}pez, Aldo Andr\'{e}s Alvarez and Meza-Ruiz, Ivan and Ortega, John E. and Palmer, Alexis and Salazar, Rodolfo Joel Zevallos and Stenzel, Kristine and Vu, Thang and Kann, Katharina},
  booktitle =      {Proceedings of the NeurIPS 2022 Competitions Track},
  pages =      {217--232},
  year =      {2022},
  editor =      {Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob},
  volume =      {220},
  series =      {Proceedings of Machine Learning Research},
  month =      {28 Nov--09 Dec},
  publisher =    {PMLR},
  pdf =      {https://proceedings.mlr.press/v220/ebrahimi22a/ebrahimi22a.pdf},
  url =      {https://proceedings.mlr.press/v220/ebrahimi22a.html},
  abstract =      {Indigenous languages, including those from the Americas, have received very little attention from the machine learning (ML) and natural language processing (NLP) communities. To tackle the resulting lack of systems for these languages and the accompanying social inequalities affecting their speakers, we conduct the second AmericasNLP competition (and the first one in collaboration with NeurIPS), which is centered around speech-to-text translation systems for Indigenous languages of the Americas. The competition features three tasks – (1) automatic speech recognition, (2) text-based machine translation, and (3) speech-to-text translation – and two tracks: constrained and unconstrained. Five Indigenous languages are covered: Bribri, Guarani, Kotiria, Wa’ikhana, and Quechua. In this overview paper, we describe the tasks, tracks, and languages, introduce the baseline and participating systems, and end with a summary of ongoing and future challenges for the automatic translation of Indigenous languages.}
}

@ARTICLE{kann2022frontiers,
    AUTHOR={Kann, Katharina and Ebrahimi, Abteen and Mager, Manuel and Oncevay, Arturo and Ortega, John E. and Rios, Annette and Fan, Angela and Gutierrez-Vasques, Ximena and Chiruzzo, Luis and Giménez-Lugo, Gustavo A. and Ramos, Ricardo and Meza Ruiz, Ivan Vladimir and Mager, Elisabeth and Chaudhary, Vishrav and Neubig, Graham and Palmer, Alexis and Coto-Solano, Rolando and Vu, Ngoc Thang},
    TITLE={AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas},
    JOURNAL={Frontiers in Artificial Intelligence},
    VOLUME={5},
    YEAR={2022},
    MONTH={DECEMBER},
    URL={https://www.frontiersin.org/articles/10.3389/frai.2022.995667},
    DOI={10.3389/frai.2022.995667},
    ISSN={2624-8212},
    ABSTRACT={Little attention has been paid to the development of human language technology for truly low-resource languages—i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.}
}


@inproceedings{kann2022emnlp,
    title = "A Major Obstacle for NLP Research: Let's Talk about Time Allocation!",
    author = "Kann, Katharina and Dudy, Shiran and McCarthy, Arya D.",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2022",
}

@inproceedings{wiemerslage2022emnlp,
    title = "A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection",
    author = "Wiemerslage, Adam and Dudy, Shiran and Kann, Katharina",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2022",
}

@inproceedings{bhatnagar2022emnlp,
    title = "CHIA: CHoosing Instances to Annotate for Machine Translation",
    author = "Bhatnagar, Rajat and Ganesh, Ananya and Kann, Katharina",
    booktitle = "Findings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2022",
}

@inproceedings{hall2022emnlp,
    title = "Generate Me a Bedtime Story: Leveraging Natural Language Processing for Early Vocabulary Enhancement",
    author = "Hall, Trevor A. and Valentini, Maria and Colunga, Eliana and Kann, Katharina",
    booktitle = "Proceedings of the Workshop on NLP for Positive Impact",
    month = december,
    year = "2022",
}

@inproceedings{kannfieldmatters2022,
    title={Machine Translation Between High-resource Languages in a Language Documentation Setting},
    year={2022},
    author={Kann, Katharina and Ebrahimi, Abteen and Stenzel, Kristine and Palmer, Alexis},
    booktitle = {Proceedings of the First Workshop on Applying NLP to Field Linguistics},
    month = october,
}

@inproceedings{ganesh2022,
    title={Response Construct Tagging: NLP-Aided Assessment for Engineering Education},
    year={2022},
    author={Ganesh, Ananya and Scribner, Hugh and Singh, Jasdeep and Goodman, Katherine and Hertzberg, Jean and Kann, Katharina},
    booktitle = {Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications},
    month = july,
}

@inproceedings{kann2022,
    title={Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next},
    year={2022},
    author={Kann, Katharina and Ebrahimi, Abteen and Koh, Joewie J. and Dudy, Shiran and Roncone, Alessandro},
    booktitle = {Proceedings of the 4th Workshop on NLP for Conversational AI},
    month = may,
    poster = {Posters/2022/ConvAI/Katharina.pdf}
}

@inproceedings{ebrahimi2022,
    title={AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages},
    year={2022},
    author={Ebrahimi, Abteen and Mager, Manuel and Oncevay, Arturo and Chaudhary, Vishrav and Chiruzzo, Luis and Fan, Angela and Ortega, John and Ramos, Ricardo and Rios, Annette and Meza Ruiz, Ivan Vladimir and Giménez-Lugo, Gustavo A. and Mager, Elisabeth and Neubig, Graham and Palmer, Alexis and  Coto-Solano, Rolando and Vu, Thang and Kann, Katharina},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may,
    video = {/assets/videos/2022/ACL/Abteen.mp4}
}

@inproceedings{fujinuma2022,
    title={How Does Multilingual Pretraining Affect Cross-Lingual Transferability?},
    year={2022},
    author={Fujinuma, Yoshinari and Boyd-Graber, Jordan Lee and Kann, Katharina},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may
}

@inproceedings{wiemerslage2022,
    title={Morphological Processing of Low-Resource Languages: Where We Are and What's Next},
    year={2022},
    author={Wiemerslage, Adam and Silfverberg, Miikka and Yang, Changbing and McCarthy, Arya D. and Nicolai, Garrett and Colunga, Eliana and Kann, Katharina},
    booktitle = {Findings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may,
    video = {/assets/videos/2022/ACL/adam.mp4},
    poster = {Posters/2022/ACL/Adam.pdf}
}

@inproceedings{mager2022,
    title={BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages},
    year={2022},
    author={Mager, Manuel and Oncevay, Arturo and Mager, Elisabeth and Kann, Katharina and Vu, Thang},
    booktitle = {Findings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may,
}

@inproceedings{paik-etal-2021-world,
    title = "{T}he {W}orld of an {O}ctopus: {H}ow {R}eporting {B}ias {I}nfluences a {L}anguage {M}odel{'}s {P}erception of {C}olor",
    author = "Paik, Cory  and
      Aroca-Ouellette, St{\'e}phane  and
      Roncone, Alessandro  and
      Kann, Katharina",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.63",
    doi = "10.18653/v1/2021.emnlp-main.63",
    pages = "823--835",
    abstract = "Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human{'}s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.",
}

@inproceedings{ganesh2021,
    title={What Would a Teacher Do? Predicting Future Talk Moves},
    year={2021},
    author={Ananya Ganesh and Martha Palmer and Katharina Kann},
    booktitle = {Findings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{arocaouellette2021,
    title={PROST: Physical Reasoning of Objects through Space and Time},
    year={2021},
    author={Stephane Aroca-Ouellette and Cory Paik and Alessandro Roncone and Katharina Kann},
    booktitle = {Findings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{ebrahimi2021-1,
    title={How to Adapt Your Pretrained Multilingual Model to 1600 Languages},
    year={2021},
    author={Abteen Ebrahimi and Katharina Kann},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{bhatnagar2021,
    title={Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data},
    year={2021},
    author={Rajat Bhatnagar and Ananya Ganesh and Katharina Kann},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{ojha-etal-2021-findings,
    title = "Findings of the {L}o{R}es{MT} 2021 Shared Task on {COVID} and Sign Language for Low-resource Languages",
    author = "Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Kann, Katharina  and
      Ortega, John  and
      Shatam, Sheetal  and
      Fransen, Theodorus",
    booktitle = "Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-loresmt.11",
    pages = "114--123",
    abstract = "We present the findings of the LoResMT 2021 shared task which focuses on machine translation (MT) of COVID-19 data for both low-resource spoken and sign languages. The organization of this task was conducted as part of the fourth workshop on technologies for machine translation of low resource languages (LoResMT). Parallel corpora is presented and publicly available which includes the following directions: English↔Irish, English↔Marathi, and Taiwanese Sign language↔Traditional Chinese. Training data consists of 8112, 20933 and 128608 segments, respectively. There are additional monolingual data sets for Marathi and English that consist of 21901 segments. The results presented here are based on entries from a total of eight teams. Three teams submitted systems for English↔Irish while five teams submitted systems for English↔Marathi. Unfortunately, there were no systems submissions for the Taiwanese Sign language↔Traditional Chinese task. Maximum system performance was computed using BLEU and follow as 36.0 for English{--}Irish, 34.6 for Irish{--}English, 24.2 for English{--}Marathi, and 31.3 for Marathi{--}English.",
}

@inproceedings{gerlach-etal-2021-paradigm,
    title = "Paradigm Clustering with Weighted Edit Distance",
    author = "Gerlach, Andrew  and
      Wiemerslage, Adam  and
      Kann, Katharina",
    booktitle = "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigmorphon-1.12",
    doi = "10.18653/v1/2021.sigmorphon-1.12",
    pages = "107--114",
    abstract = "This paper describes our system for the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering, which asks participants to group inflected forms together according their underlying lemma without the aid of annotated training data. We employ agglomerative clustering to group word forms together using a metric that combines an orthographic distance and a semantic distance from word embeddings. We experiment with two variations of an edit distance-based model for quantifying orthographic distance, but, due to time constraints, our system does not improve over the shared task{'}s baseline system.",
}

@inproceedings{wiemerslage-etal-2021-findings,
    title = "Findings of the {SIGMORPHON} 2021 Shared Task on Unsupervised Morphological Paradigm Clustering",
    author = "Wiemerslage, Adam  and
      McCarthy, Arya D.  and
      Erdmann, Alexander  and
      Nicolai, Garrett  and
      Agirrezabal, Manex  and
      Silfverberg, Miikka  and
      Hulden, Mans  and
      Kann, Katharina",
    booktitle = "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigmorphon-1.8",
    doi = "10.18653/v1/2021.sigmorphon-1.8",
    pages = "72--81",
    abstract = "We describe the second SIGMORPHON shared task on unsupervised morphology: the goal of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering is to cluster word types from a raw text corpus into paradigms. To this end, we release corpora for 5 development and 9 test languages, as well as gold partial paradigms for evaluation. We receive 14 submissions from 4 teams that follow different strategies, and the best performing system is based on adaptor grammars. Results vary significantly across languages. However, all systems are outperformed by a supervised lemmatizer, implying that there is still room for improvement.",
}

@inproceedings{mager2021,
    title = {Findings of the {A}mericas{NLP} 2021 Shared Task on Open Machine Translation for Indigenous Languages of the {A}mericas},
    author = {Mager, Manuel  and
      Oncevay, Arturo  and
      Ebrahimi, Abteen  and
      Ortega, John  and
      Rios, Annette  and
      Fan, Angela  and
      Gutierrez-Vasques, Ximena  and
      Chiruzzo, Luis  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Ramos, Ricardo  and
      Meza Ruiz, Ivan Vladimir  and
      Coto-Solano, Rolando  and
      Palmer, Alexis  and
      Mager-Hois, Elisabeth  and
      Chaudhary, Vishrav  and
      Neubig, Graham  and
      Vu, Ngoc Thang  and
      Kann, Katharina},
    year={2021},
    booktitle = "Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas",
    month = jun,
}

@inproceedings{kann_monsalve2021,
    title={Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings},
    author={Katharina Kann and Mauro M. Monsalve-Mercado},
    year={2021},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics",
    month = apr,
}

@inproceedings{beilei2021,
    title={CLiMP: A Benchmark for Chinese Language Model Evaluation},
    year={2021},
    author={Beilei Xiang and Changbing Yang and Yu Li and Alex Warstadt and Katharina Kann},
    booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics},
    month = apr,
}

@inproceedings{nikhil2020,
    title={Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies},
    author={Nikhil Prabhu and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing Student Research Workshop",
    month = dec,
    info={Best Paper Award}
}

@inproceedings{phang2020english,
    title={English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too},
    author={Jason Phang and Phu Mon Htut and Yada Pruksachatkun and Haokun Liu and Clara Vania and Iacer Calixto and Katharina Kann and Samuel R. Bowman},
    year={2020},
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing",
    month = dec,
}

@inproceedings{mager2020,
    title={Tackling the Low-resource Challenge for Canonical Segmentation},
    author={Manuel Mager and Özlem Çetinoğlu and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{agarwal2020,
    title={Acrostic Poem Generation},
    author={Rajat Agarwal and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{moeller2020,
    title={IGT2P: From Interlinear Glossed Texts to Paradigms},
    author={Sarah Moeller and Ling Liu and Changbing Yang and Katharina Kann and Mans Hulden},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{zhang-etal-2020-overfitting,
    title = "Why Overfitting Isn{'}t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries",
    author = "Zhang, Mozhi  and
      Fujinuma, Yoshinari  and
      Paul, Michael J.  and
      Boyd-Graber, Jordan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = {2020},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.201",
    doi = "10.18653/v1/2020.acl-main.201",
    pages = "2214--2220",
    abstract = "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.",
}

@inproceedings{kann-etal-2020-sigmorphon,
    title = "The {SIGMORPHON} 2020 Shared Task on Unsupervised Morphological Paradigm Completion",
    author = "Kann, Katharina  and
      McCarthy, Arya D.  and
      Nicolai, Garrett  and
      Hulden, Mans",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.3",
    doi = "10.18653/v1/2020.sigmorphon-1.3",
    pages = "51--62",
    abstract = "In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology. Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma. In order to simulate a realistic use case, we first released data for 5 development languages. However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline. We provided a modular baseline system, which is a pipeline of 4 components. 3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic.",
}

@inproceedings{prabhu-kann-2020-frustratingly,
    title = "Frustratingly Easy Multilingual Grapheme-to-Phoneme Conversion",
    author = "Prabhu, Nikhil  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.13",
    doi = "10.18653/v1/2020.sigmorphon-1.13",
    pages = "123--127",
    abstract = "In this paper, we describe two CU-Boulder submissions to the SIGMORPHON 2020 Task 1 on multilingual grapheme-to-phoneme conversion (G2P). Inspired by the high performance of a standard transformer model (Vaswani et al., 2017) on the task, we improve over this approach by adding two modifications: (i) Instead of training exclusively on G2P, we additionally create examples for the opposite direction, phoneme-to-grapheme conversion (P2G). We then perform multi-task training on both tasks. (ii) We produce ensembles of our models via majority voting. Our approaches, though being conceptually simple, result in systems that place 6th and 8th amongst 23 submitted systems, and obtain the best results out of all systems on Lithuanian and Modern Greek, respectively.",
}

@inproceedings{singer-kann-2020-nyu,
    title = "The {NYU}-{CUB}oulder Systems for {SIGMORPHON} 2020 Task 0 and Task 2",
    author = "Singer, Assaf  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.8",
    doi = "10.18653/v1/2020.sigmorphon-1.8",
    pages = "90--98",
    abstract = "We describe the NYU-CUBoulder systems for the SIGMORPHON 2020 Task 0 on typologically diverse morphological inflection and Task 2 on unsupervised morphological paradigm completion. The former consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. The latter requires generating entire paradigms for a set of given lemmas from raw text alone. We model morphological inflection as a sequence-to-sequence problem, where the input is the sequence of the lemma{'}s characters with morphological tags, and the output is the sequence of the inflected form{'}s characters. First, we apply a transformer model to the task. Second, as inflected forms share most characters with the lemma, we further propose a pointer-generator transformer model to allow easy copying of input characters.",
}

@inproceedings{mager-kann-2020-ims,
    title = "The {IMS}{--}{CUB}oulder System for the {SIGMORPHON} 2020 Shared Task on Unsupervised Morphological Paradigm Completion",
    author = "Mager, Manuel  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.9",
    doi = "10.18653/v1/2020.sigmorphon-1.9",
    pages = "99--105",
    abstract = "In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMS--CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.",
}


@inproceedings{mohananey-etal-2020-self,
    title = "Self-Training for Unsupervised Parsing with {PRPN}",
    author = "Mohananey, Anhad  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.iwpt-1.11",
    doi = "10.18653/v1/2020.iwpt-1.11",
    pages = "105--110",
    abstract = "Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like language modeling. In this work, we propose self-training for neural UP models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. To be able to use our model{'}s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with parses predicted by our model to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1{\%} F1 and the previous state of the art by 1.6{\%} F1. In addition, we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings.",
}

@inproceedings{pruksachatkun-etal-2020-intermediate,
    title = "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
    author = "Pruksachatkun, Yada  and
      Phang, Jason  and
      Liu, Haokun  and
      Htut, Phu Mon  and
      Zhang, Xiaoyi  and
      Pang, Richard Yuanzhe  and
      Vania, Clara  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.467",
    doi = "10.18653/v1/2020.acl-main.467",
    pages = "5231--5247",
    abstract = "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
}

@inproceedings{jin-etal-2020-unsupervised,
    title = "Unsupervised Morphological Paradigm Completion",
    author = "Jin, Huiming  and
      Cai, Liwei  and
      Peng, Yihui  and
      Xia, Chen  and
      McCarthy, Arya  and
      Kann, Katharina",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.598",
    doi = "10.18653/v1/2020.acl-main.598",
    pages = "6696--6707",
    abstract = "We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.",
}

@inproceedings{kann2020learning,
    title={Learning to Learn Morphological Inflection for Resource-Poor Languages},
    author={Katharina Kann and Samuel R. Bowman and Kyunghyun Cho},
    year={2020},
    booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence}
}

@inproceedings{kann2020weakly,
    title={Weakly Supervised POS Taggers Perform Poorly on Truly Low-Resource Languages},
    author={Katharina Kann and Ophélie Lacroix and Anders Søgaard},
    year={2020},
    booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence}
}

@inproceedings{kann-2020-acquisition,
    title = "Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge",
    author = "Kann, Katharina",
    booktitle = "Proceedings of the Society for Computation in Linguistics",
    month = jan,
    year = "2020",
    address = "New York, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.scil-1.19",
    pages = "129--138",
}
