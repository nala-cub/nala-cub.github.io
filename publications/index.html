<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>NALA  Group | publications</title>
<meta name="description" content="Website of CU Boulder's NALA Group
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://nala-cub.github.io/">
       <span class="font-weight-bold">NALA</span>   Group
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/joining_us/">
                joining us
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/resources/">
                resources
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/team/">
                team
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/values/">
                values
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">Selected publications of NALA members (since 2020).</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="shaier2024findingsacl" class="col-sm-8">
    
      <div class="title">It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension</div>
      <div class="author">
        
          
            
              
                <em>Sagi Shaier</em>,
              
            
          
        
          
            
              
                
                  Lawrence Hunter,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the Association for Computational Linguistics ACL 2024</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="rice2024acl" class="col-sm-8">
    
      <div class="title">TAMS: Translation-Assisted Morphological Segmentation</div>
      <div class="author">
        
          
            
              
                <em>Enora Rice</em>,
              
            
          
        
          
            
              
                
                  Ali Marashian,
                
              
            
          
        
          
            
              
                <em>Luke Gessler</em>,
              
            
          
        
          
            
              
                
                  Alexis Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="french-etal-2024-aligning" class="col-sm-8">
    
      <div class="title">Aligning to Adults Is Easy, Aligning to Children Is Hard: A Study of Linguistic Alignment in Dialogue Systems</div>
      <div class="author">
        
          
            
              
                <em>Dorothea French</em>,
              
            
          
        
          
            
              
                
                  Sidney Dâ€™Mello,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 1st Human-Centered Large Language Modeling Workshop</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hulle2024" class="col-sm-8">
    
      <div class="title">Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent</div>
      <div class="author">
        
          
            
              
                
                  Nikhil Hulle,
                
              
            
          
        
          
            
              
                <em>Stephane Aroca-Ouellette</em>,
              
            
          
        
          
            
              
                
                  Anthony Ries,
                
              
            
          
        
          
            
              
                
                  Jake Brawer,
                
              
            
          
        
          
            
              
                <em>Katharina von der Wense</em>,
              
            
          
        
          
            
              
                
                  and Alessandro Roncone
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ebrahimi2024americasnlp" class="col-sm-8">
    
      <div class="title">Findings of the AmericasNLP 2024 Shared Task on Machine Translation into Indigenous Languages</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Ona Gibert,
                
              
            
          
        
          
            
              
                
                  Raul Vazquez,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Pavel Denisov,
                
              
            
          
        
          
            
              
                
                  Robert Pugh,
                
              
            
          
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                <em>Katharina von der Wense</em>,
              
            
          
        
          
            
              
                
                  and Shruti Rijhwani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="chiruzzo2024americasnlp" class="col-sm-8">
    
      <div class="title">Findings of the AmericasNLP 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages</div>
      <div class="author">
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                
                  Pavel Denisov,
                
              
            
          
        
          
            
              
                
                  Alejandro Molina-Villegas,
                
              
            
          
        
          
            
              
                
                  Silvia Fernandez-Sabido,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Marvin AgÃ¼ero-Torales,
                
              
            
          
        
          
            
              
                
                  Aldo Alvarez,
                
              
            
          
        
          
            
              
                
                  Samuel Canul-Yah,
                
              
            
          
        
          
            
              
                
                  Lorena Hau-UcÃ¡n,
                
              
            
          
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Robert Pugh,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  Shruti Rijhwani,
                
              
            
          
        
          
            
              
                <em>Katharina von der Wense</em>,
              
            
          
        
          
            
              
                
                  and Manuel Mager
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="weber2024cogsci" class="col-sm-8">
    
      <div class="title">Evaluating LLMs as Tools to Support Early Vocabulary Learning</div>
      <div class="author">
        
          
            
              
                
                  Jennifer Weber,
                
              
            
          
        
          
            
              
                <em>Maria Valentini</em>,
              
            
          
        
          
            
              
                <em>TÃ©a Wright</em>,
              
            
          
        
          
            
              
                <em>Katharina von der Wense</em>,
              
            
          
        
          
            
              
                
                  and Eliana Colunga
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Annual Meeting of the Cognitive Science Society (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2024edm" class="col-sm-8">
    
      <div class="title">Prompting as Panacea? A Case Study of In-Context Learning Performance for Qualitative Coding of Classroom Dialog</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Chelsea Chandler,
                
              
            
          
        
          
            
              
                
                  Sidney Dâ€™Mello,
                
              
            
          
        
          
            
              
                
                  Martha Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the International Conference on Educational Data Mining (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ebrahimi2024naacl" class="col-sm-8">
    
      <div class="title">Zero-Shot vs. Translation-Based Cross-Lingual Transfer: The Case of Lexical Gaps</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bui2024negnlp" class="col-sm-8">
    
      <div class="title">Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget</div>
      <div class="author">
        
          
            
              
                <em>Minh Duc Bui</em>,
              
            
          
        
          
            
              
                
                  Fabian David Schmidt,
                
              
            
          
        
          
            
              
                
                  Goran GlavaÅ¡,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Workshop on Insights from Negative Results in NLP (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bui2024trustnlp" class="col-sm-8">
    
      <div class="title">The Trade-off between Performance, Efficiency, and Fairness in Adapter Modules for Text Classification</div>
      <div class="author">
        
          
            
              
                <em>Minh Duc Bui</em>,
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Fourth Workshop on Trustworthy Natural Language Processing (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="gessler2024americasnlp" class="col-sm-8">
    
      <div class="title">NLP for Language Documentation: Two Reasons for the Gap between Theory and Practice</div>
      <div class="author">
        
          
            
              
                <em>Luke Gessler</em>,
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP) (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bui2024americasnlpst" class="col-sm-8">
    
      <div class="title">JGU Mainzâ€™s Submission to the AmericasNLP 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages</div>
      <div class="author">
        
          
            
              
                <em>Minh Duc Bui</em>,
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP) (to appear)</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="wiemerslage2024eacl" class="col-sm-8">
    
      <div class="title">Quantifying the Hyperparameter Sensitivity of Neural Networks for Character-level Sequence-to-Sequence Tasks</div>
      <div class="author">
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                
                  Kyle Gorman,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="shaier2024eacl1" class="col-sm-8">
    
      <div class="title">Comparing Template-based and Template-free Language Model Probing</div>
      <div class="author">
        
          
            
              
                <em>Sagi Shaier</em>,
              
            
          
        
          
            
              
                
                  Kevin Bennett,
                
              
            
          
        
          
            
              
                
                  Lawrence Hunter,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="shaier2024eacl2" class="col-sm-8">
    
      <div class="title">Desiderata For The Context Use Of Question Answering Systems</div>
      <div class="author">
        
          
            
              
                <em>Sagi Shaier</em>,
              
            
          
        
          
            
              
                
                  Lawrence Hunter,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>,
      
      
        2024
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="valentini2023emnlp" class="col-sm-8">
    
      <div class="title">On the Automatic Generation and Simplification of Childrenâ€™s Stories</div>
      <div class="author">
        
          
            
              
                <em>Maria Valentini</em>,
              
            
          
        
          
            
              
                
                  Jennifer Weber,
                
              
            
          
        
          
            
              
                <em>Jesus Salcido</em>,
              
            
          
        
          
            
              
                <em>TÃ©a Wright</em>,
              
            
          
        
          
            
              
                
                  Eliana Colunga,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="shaier2023aacl2" class="col-sm-8">
    
      <div class="title">Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</div>
      <div class="author">
        
          
            
              
                <em>Sagi Shaier</em>,
              
            
          
        
          
            
              
                
                  Kevin Bennett,
                
              
            
          
        
          
            
              
                
                  Lawrence Hunter,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="shaier2023aacl1" class="col-sm-8">
    
      <div class="title">Who Are All The Stochastic Parrots Imitating? They Should Tell Us!</div>
      <div class="author">
        
          
            
              
                <em>Sagi Shaier</em>,
              
            
          
        
          
            
              
                
                  Lawrence Hunter,
                
              
            
          
        
          
            
              
                and <em>Katharina von der Wense</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2023coco4mt" class="col-sm-8">
    
      <div class="title">Findings of the CoCo4MT 2023 Shared Task on Corpus Construction for Machine Translation</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Marine Carpuat,
                
              
            
          
        
          
            
              
                
                  William Chen,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  Constantine Lignos,
                
              
            
          
        
          
            
              
                
                  John E. Ortega,
                
              
            
          
        
          
            
              
                
                  Jonne Saleva,
                
              
            
          
        
          
            
              
                
                  Shabnam Tafreshi,
                
              
            
          
        
          
            
              
                
                  and Rodolfo Zevallos
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Second Workshop on Corpus Generation and Corpus Augmentation for Machine Translation</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mager2023americasnlp" class="col-sm-8">
    
      <div class="title">Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction</div>
      <div class="author">
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                <em>Rajat Bhatnagar</em>,
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Ngoc Thang Vu,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Third Workshop on NLP for Indigenous Languages of the Americas</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ebrahimi2023americasnlp" class="col-sm-8">
    
      <div class="title">Findings of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Shruti Rijhwani,
                
              
            
          
        
          
            
              
                <em>Enora Rice</em>,
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  Claudia Baltazar,
                
              
            
          
        
          
            
              
                
                  MarÃ­a CortÃ©s,
                
              
            
          
        
          
            
              
                
                  Cynthia MontaÃ±o,
                
              
            
          
        
          
            
              
                
                  John E Ortega,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Hilaria Cruz,
                
              
            
          
        
          
            
              
                
                  Alexis Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Third Workshop on NLP for Indigenous Languages of the Americas</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2023convai" class="col-sm-8">
    
      <div class="title">A Survey of Challenges and Methods in the Computational Modeling of Multi-Party Dialog</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Martha Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 5th Workshop on NLP for Conversational AI</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2023acl" class="col-sm-8">
    
      <div class="title">Mind the Gap between the Application Track and the Real World</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Jie Cao,
                
              
            
          
        
          
            
              
                
                  Margaret Perkoff,
                
              
            
          
        
          
            
              
                
                  Rosy Southwell,
                
              
            
          
        
          
            
              
                
                  Martha Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mager2023acl" class="col-sm-8">
    
      <div class="title">Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers</div>
      <div class="author">
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Elisabeth Albine Mager,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Ngoc Thang Vu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="wiemerslage2023acl" class="col-sm-8">
    
      <div class="title">An Investigation of Noise in Morphological Inflection</div>
      <div class="author">
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                
                  Changbing Yang,
                
              
            
          
        
          
            
              
                
                  Garrett Nicolai,
                
              
            
          
        
          
            
              
                
                  Miikka Silfverberg,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the 61st Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="jie2023umap" class="col-sm-8">
    
      <div class="title">A Comparative Analysis of Automatic Speech Recognition Errors in Small Group Classroom Discourse</div>
      <div class="author">
        
          
            
              
                
                  Jie Cao,
                
              
            
          
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Jon Cai,
                
              
            
          
        
          
            
              
                
                  Rosy Southwell,
                
              
            
          
        
          
            
              
                
                  Margaret Perkoff,
                
              
            
          
        
          
            
              
                
                  Michael Reagan,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  James Martin,
                
              
            
          
        
          
            
              
                
                  Martha Palmer,
                
              
            
          
        
          
            
              
                
                  and Sidney Dâ€™Mello
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2023aied" class="col-sm-8">
    
      <div class="title">Navigating Wanderland: Highlighting Off-Task Discussions in Classrooms</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Michael Chang,
                
              
            
          
        
          
            
              
                
                  Rachel Dickler,
                
              
            
          
        
          
            
              
                
                  Michael Regan,
                
              
            
          
        
          
            
              
                
                  Jon Cai,
                
              
            
          
        
          
            
              
                
                  Kristin Wright-Bettner,
                
              
            
          
        
          
            
              
                
                  James Pustejovsky,
                
              
            
          
        
          
            
              
                
                  James Martin,
                
              
            
          
        
          
            
              
                
                  Jeff Flanigan,
                
              
            
          
        
          
            
              
                
                  Martha Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 24th International Conference on Artificial Intelligence in Education</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ebrahimieacl2023" class="col-sm-8">
    
      <div class="title">Meeting the Needs of Low-Resource Languages: Exploring Automatic Alignments via Pretrained Models</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Arya D. McCarthy,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  John E. Ortega,
                
              
            
          
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Gustavo A. GimÃ©nez-Lugo,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>,
      
      
        2023
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="pmlr-v220-ebrahimi22a" class="col-sm-8">
    
      <div class="title">Findings of the Second AmericasNLP Competition on Speech-to-Text Translation</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                
                  Pavel Denisov,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  Danni Liu,
                
              
            
          
        
          
            
              
                
                  Sai Koneru,
                
              
            
          
        
          
            
              
                
                  Enes Yavuz Ugan,
                
              
            
          
        
          
            
              
                
                  Zhaolin Li,
                
              
            
          
        
          
            
              
                
                  Jan Niehues,
                
              
            
          
        
          
            
              
                
                  Monica Romero,
                
              
            
          
        
          
            
              
                
                  Ivan G Torre,
                
              
            
          
        
          
            
              
                
                  Tanel AlumÃ¤e,
                
              
            
          
        
          
            
              
                
                  Jiaming Kong,
                
              
            
          
        
          
            
              
                
                  Sergey Polezhaev,
                
              
            
          
        
          
            
              
                
                  Yury Belousov,
                
              
            
          
        
          
            
              
                
                  Wei-Rui Chen,
                
              
            
          
        
          
            
              
                
                  Peter Sullivan,
                
              
            
          
        
          
            
              
                
                  Ife Adebara,
                
              
            
          
        
          
            
              
                
                  Bashar Talafha,
                
              
            
          
        
          
            
              
                
                  Alcides Alcoba Inciarte,
                
              
            
          
        
          
            
              
                
                  Muhammad Abdul-Mageed,
                
              
            
          
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Hilaria Cruz,
                
              
            
          
        
          
            
              
                
                  SofÃ­a Flores-SolÃ³rzano,
                
              
            
          
        
          
            
              
                
                  Aldo AndrÃ©s Alvarez LÃ³pez,
                
              
            
          
        
          
            
              
                
                  Ivan Meza-Ruiz,
                
              
            
          
        
          
            
              
                
                  John E. Ortega,
                
              
            
          
        
          
            
              
                
                  Alexis Palmer,
                
              
            
          
        
          
            
              
                
                  Rodolfo Joel Zevallos Salazar,
                
              
            
          
        
          
            
              
                
                  Kristine Stenzel,
                
              
            
          
        
          
            
              
                
                  Thang Vu,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the NeurIPS 2022 Competitions Track</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://proceedings.mlr.press/v220/ebrahimi22a/ebrahimi22a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Indigenous languages, including those from the Americas, have received very little attention from the machine learning (ML) and natural language processing (NLP) communities. To tackle the resulting lack of systems for these languages and the accompanying social inequalities affecting their speakers, we conduct the second AmericasNLP competition (and the first one in collaboration with NeurIPS), which is centered around speech-to-text translation systems for Indigenous languages of the Americas. The competition features three tasks â€“ (1) automatic speech recognition, (2) text-based machine translation, and (3) speech-to-text translation â€“ and two tracks: constrained and unconstrained. Five Indigenous languages are covered: Bribri, Guarani, Kotiria, Waâ€™ikhana, and Quechua. In this overview paper, we describe the tasks, tracks, and languages, introduce the baseline and participating systems, and end with a summary of ongoing and future challenges for the automatic translation of Indigenous languages.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann2022frontiers" class="col-sm-8">
    
      <div class="title">AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  John E. Ortega,
                
              
            
          
        
          
            
              
                
                  Annette Rios,
                
              
            
          
        
          
            
              
                
                  Angela Fan,
                
              
            
          
        
          
            
              
                
                  Ximena Gutierrez-Vasques,
                
              
            
          
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                
                  Gustavo A. GimÃ©nez-Lugo,
                
              
            
          
        
          
            
              
                
                  Ricardo Ramos,
                
              
            
          
        
          
            
              
                
                  Ivan Vladimir Meza Ruiz,
                
              
            
          
        
          
            
              
                
                  Elisabeth Mager,
                
              
            
          
        
          
            
              
                
                  Vishrav Chaudhary,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Alexis Palmer,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  and Ngoc Thang Vu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in Artificial Intelligence</em>
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Little attention has been paid to the development of human language technology for truly low-resource languagesâ€”i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann2022emnlp" class="col-sm-8">
    
      <div class="title">A Major Obstacle for NLP Research: Letâ€™s Talk about Time Allocation!</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                <em>Shiran Dudy</em>,
              
            
          
        
          
            
              
                
                  and Arya D. McCarthy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="wiemerslage2022emnlp" class="col-sm-8">
    
      <div class="title">A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection</div>
      <div class="author">
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                <em>Shiran Dudy</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bhatnagar2022emnlp" class="col-sm-8">
    
      <div class="title">CHIA: CHoosing Instances to Annotate for Machine Translation</div>
      <div class="author">
        
          
            
              
                <em>Rajat Bhatnagar</em>,
              
            
          
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hall2022emnlp" class="col-sm-8">
    
      <div class="title">Generate Me a Bedtime Story: Leveraging Natural Language Processing for Early Vocabulary Enhancement</div>
      <div class="author">
        
          
            
              
                <em>Trevor A. Hall</em>,
              
            
          
        
          
            
              
                <em>Maria Valentini</em>,
              
            
          
        
          
            
              
                
                  Eliana Colunga,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Workshop on NLP for Positive Impact</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kannfieldmatters2022" class="col-sm-8">
    
      <div class="title">Machine Translation Between High-resource Languages in a Language Documentation Setting</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Kristine Stenzel,
                
              
            
          
        
          
            
              
                
                  and Alexis Palmer
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the First Workshop on Applying NLP to Field Linguistics</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2022" class="col-sm-8">
    
      <div class="title">Response Construct Tagging: NLP-Aided Assessment for Engineering Education</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Hugh Scribner,
                
              
            
          
        
          
            
              
                <em>Jasdeep Singh</em>,
              
            
          
        
          
            
              
                
                  Katherine Goodman,
                
              
            
          
        
          
            
              
                
                  Jean Hertzberg,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann2022" class="col-sm-8">
    
      <div class="title">Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Joewie J. Koh,
                
              
            
          
        
          
            
              
                <em>Shiran Dudy</em>,
              
            
          
        
          
            
              
                
                  and Alessandro Roncone
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 4th Workshop on NLP for Conversational AI</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
      
      <a href="/assets/pdf/Posters/2022/ConvAI/Katharina.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ebrahimi2022" class="col-sm-8">
    
      <div class="title">AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  Vishrav Chaudhary,
                
              
            
          
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                
                  Angela Fan,
                
              
            
          
        
          
            
              
                
                  John Ortega,
                
              
            
          
        
          
            
              
                
                  Ricardo Ramos,
                
              
            
          
        
          
            
              
                
                  Annette Rios,
                
              
            
          
        
          
            
              
                
                  Ivan Vladimir Meza Ruiz,
                
              
            
          
        
          
            
              
                
                  Gustavo A. GimÃ©nez-Lugo,
                
              
            
          
        
          
            
              
                
                  Elisabeth Mager,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Alexis Palmer,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Thang Vu,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
      <a href="/assets/videos/2022/ACL/Abteen.mp4" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="fujinuma2022" class="col-sm-8">
    
      <div class="title">How Does Multilingual Pretraining Affect Cross-Lingual Transferability?</div>
      <div class="author">
        
          
            
              
                <em>Yoshinari Fujinuma</em>,
              
            
          
        
          
            
              
                
                  Jordan Lee Boyd-Graber,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="wiemerslage2022" class="col-sm-8">
    
      <div class="title">Morphological Processing of Low-Resource Languages: Where We Are and Whatâ€™s Next</div>
      <div class="author">
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                
                  Miikka Silfverberg,
                
              
            
          
        
          
            
              
                
                  Changbing Yang,
                
              
            
          
        
          
            
              
                
                  Arya D. McCarthy,
                
              
            
          
        
          
            
              
                
                  Garrett Nicolai,
                
              
            
          
        
          
            
              
                
                  Eliana Colunga,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the 60th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
      
      <a href="/assets/pdf/Posters/2022/ACL/Adam.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
      <a href="/assets/videos/2022/ACL/adam.mp4" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mager2022" class="col-sm-8">
    
      <div class="title">BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages</div>
      <div class="author">
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                
                  Elisabeth Mager,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Thang Vu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the 60th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2022
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="paik-etal-2021-world" class="col-sm-8">
    
      <div class="title">The World of an Octopus: How Reporting Bias Influences a Language Modelâ€™s Perception of Color</div>
      <div class="author">
        
          
            
              
                <em>Cory Paik</em>,
              
            
          
        
          
            
              
                <em>StÃ©phane Aroca-Ouellette</em>,
              
            
          
        
          
            
              
                
                  Alessandro Roncone,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a humanâ€™s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganesh2021" class="col-sm-8">
    
      <div class="title">What Would a Teacher Do? Predicting Future Talk Moves</div>
      <div class="author">
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                
                  Martha Palmer,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the 59th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="arocaouellette2021" class="col-sm-8">
    
      <div class="title">PROST: Physical Reasoning of Objects through Space and Time</div>
      <div class="author">
        
          
            
              
                <em>Stephane Aroca-Ouellette</em>,
              
            
          
        
          
            
              
                <em>Cory Paik</em>,
              
            
          
        
          
            
              
                
                  Alessandro Roncone,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Findings of the 59th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ebrahimi2021-1" class="col-sm-8">
    
      <div class="title">How to Adapt Your Pretrained Multilingual Model to 1600 Languages</div>
      <div class="author">
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bhatnagar2021" class="col-sm-8">
    
      <div class="title">Donâ€™t Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data</div>
      <div class="author">
        
          
            
              
                <em>Rajat Bhatnagar</em>,
              
            
          
        
          
            
              
                <em>Ananya Ganesh</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ojha-etal-2021-findings" class="col-sm-8">
    
      <div class="title">Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages</div>
      <div class="author">
        
          
            
              
                
                  Atul Kr. Ojha,
                
              
            
          
        
          
            
              
                
                  Chao-Hong Liu,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  John Ortega,
                
              
            
          
        
          
            
              
                
                  Sheetal Shatam,
                
              
            
          
        
          
            
              
                
                  and Theodorus Fransen
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present the findings of the LoResMT 2021 shared task which focuses on machine translation (MT) of COVID-19 data for both low-resource spoken and sign languages. The organization of this task was conducted as part of the fourth workshop on technologies for machine translation of low resource languages (LoResMT). Parallel corpora is presented and publicly available which includes the following directions: Englishâ†”Irish, Englishâ†”Marathi, and Taiwanese Sign languageâ†”Traditional Chinese. Training data consists of 8112, 20933 and 128608 segments, respectively. There are additional monolingual data sets for Marathi and English that consist of 21901 segments. The results presented here are based on entries from a total of eight teams. Three teams submitted systems for Englishâ†”Irish while five teams submitted systems for Englishâ†”Marathi. Unfortunately, there were no systems submissions for the Taiwanese Sign languageâ†”Traditional Chinese task. Maximum system performance was computed using BLEU and follow as 36.0 for Englishâ€“Irish, 34.6 for Irishâ€“English, 24.2 for Englishâ€“Marathi, and 31.3 for Marathiâ€“English.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="gerlach-etal-2021-paradigm" class="col-sm-8">
    
      <div class="title">Paradigm Clustering with Weighted Edit Distance</div>
      <div class="author">
        
          
            
              
                <em>Andrew Gerlach</em>,
              
            
          
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper describes our system for the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering, which asks participants to group inflected forms together according their underlying lemma without the aid of annotated training data. We employ agglomerative clustering to group word forms together using a metric that combines an orthographic distance and a semantic distance from word embeddings. We experiment with two variations of an edit distance-based model for quantifying orthographic distance, but, due to time constraints, our system does not improve over the shared taskâ€™s baseline system.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="wiemerslage-etal-2021-findings" class="col-sm-8">
    
      <div class="title">Findings of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering</div>
      <div class="author">
        
          
            
              
                <em>Adam Wiemerslage</em>,
              
            
          
        
          
            
              
                
                  Arya D. McCarthy,
                
              
            
          
        
          
            
              
                
                  Alexander Erdmann,
                
              
            
          
        
          
            
              
                
                  Garrett Nicolai,
                
              
            
          
        
          
            
              
                
                  Manex Agirrezabal,
                
              
            
          
        
          
            
              
                
                  Miikka Silfverberg,
                
              
            
          
        
          
            
              
                
                  Mans Hulden,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe the second SIGMORPHON shared task on unsupervised morphology: the goal of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering is to cluster word types from a raw text corpus into paradigms. To this end, we release corpora for 5 development and 9 test languages, as well as gold partial paradigms for evaluation. We receive 14 submissions from 4 teams that follow different strategies, and the best performing system is based on adaptor grammars. Results vary significantly across languages. However, all systems are outperformed by a supervised lemmatizer, implying that there is still room for improvement.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mager2021" class="col-sm-8">
    
      <div class="title">Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas</div>
      <div class="author">
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Arturo Oncevay,
                
              
            
          
        
          
            
              
                <em>Abteen Ebrahimi</em>,
              
            
          
        
          
            
              
                
                  John Ortega,
                
              
            
          
        
          
            
              
                
                  Annette Rios,
                
              
            
          
        
          
            
              
                
                  Angela Fan,
                
              
            
          
        
          
            
              
                
                  Ximena Gutierrez-Vasques,
                
              
            
          
        
          
            
              
                
                  Luis Chiruzzo,
                
              
            
          
        
          
            
              
                
                  Gustavo GimÃ©nez-Lugo,
                
              
            
          
        
          
            
              
                
                  Ricardo Ramos,
                
              
            
          
        
          
            
              
                
                  Ivan Vladimir Meza Ruiz,
                
              
            
          
        
          
            
              
                
                  Rolando Coto-Solano,
                
              
            
          
        
          
            
              
                
                  Alexis Palmer,
                
              
            
          
        
          
            
              
                
                  Elisabeth Mager-Hois,
                
              
            
          
        
          
            
              
                
                  Vishrav Chaudhary,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Ngoc Thang Vu,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann_monsalve2021" class="col-sm-8">
    
      <div class="title">Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Mauro M. Monsalve-Mercado
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="beilei2021" class="col-sm-8">
    
      <div class="title">CLiMP: A Benchmark for Chinese Language Model Evaluation</div>
      <div class="author">
        
          
            
              
                
                  Beilei Xiang,
                
              
            
          
        
          
            
              
                
                  Changbing Yang,
                
              
            
          
        
          
            
              
                
                  Yu Li,
                
              
            
          
        
          
            
              
                
                  Alex Warstadt,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</em>,
      
      
        2021
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="nikhil2020" class="col-sm-8">
    
      <div class="title">Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies</div>
      <div class="author">
        
          
            
              
                <em>Nikhil Prabhu</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing Student Research Workshop</em>,
      
      
        2020
      
      </div>
    
    
    
      <div><b>Best Paper Award</b></div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="phang2020english" class="col-sm-8">
    
      <div class="title">English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too</div>
      <div class="author">
        
          
            
              
                
                  Jason Phang,
                
              
            
          
        
          
            
              
                
                  Phu Mon Htut,
                
              
            
          
        
          
            
              
                
                  Yada Pruksachatkun,
                
              
            
          
        
          
            
              
                
                  Haokun Liu,
                
              
            
          
        
          
            
              
                
                  Clara Vania,
                
              
            
          
        
          
            
              
                
                  Iacer Calixto,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Samuel R. Bowman
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mager2020" class="col-sm-8">
    
      <div class="title">Tackling the Low-resource Challenge for Canonical Segmentation</div>
      <div class="author">
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                
                  Ã–zlem Ã‡etinoÄŸlu,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="agarwal2020" class="col-sm-8">
    
      <div class="title">Acrostic Poem Generation</div>
      <div class="author">
        
          
            
              
                
                  Rajat Agarwal,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="moeller2020" class="col-sm-8">
    
      <div class="title">IGT2P: From Interlinear Glossed Texts to Paradigms</div>
      <div class="author">
        
          
            
              
                
                  Sarah Moeller,
                
              
            
          
        
          
            
              
                
                  Ling Liu,
                
              
            
          
        
          
            
              
                
                  Changbing Yang,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Mans Hulden
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="zhang-etal-2020-overfitting" class="col-sm-8">
    
      <div class="title">Why Overfitting Isnâ€™t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries</div>
      <div class="author">
        
          
            
              
                
                  Mozhi Zhang,
                
              
            
          
        
          
            
              
                <em>Yoshinari Fujinuma</em>,
              
            
          
        
          
            
              
                
                  Michael J. Paul,
                
              
            
          
        
          
            
              
                
                  and Jordan Boyd-Graber
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann-etal-2020-sigmorphon" class="col-sm-8">
    
      <div class="title">The SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  Arya D. McCarthy,
                
              
            
          
        
          
            
              
                
                  Garrett Nicolai,
                
              
            
          
        
          
            
              
                
                  and Mans Hulden
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology. Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma. In order to simulate a realistic use case, we first released data for 5 development languages. However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline. We provided a modular baseline system, which is a pipeline of 4 components. 3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="prabhu-kann-2020-frustratingly" class="col-sm-8">
    
      <div class="title">Frustratingly Easy Multilingual Grapheme-to-Phoneme Conversion</div>
      <div class="author">
        
          
            
              
                <em>Nikhil Prabhu</em>,
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we describe two CU-Boulder submissions to the SIGMORPHON 2020 Task 1 on multilingual grapheme-to-phoneme conversion (G2P). Inspired by the high performance of a standard transformer model (Vaswani et al., 2017) on the task, we improve over this approach by adding two modifications: (i) Instead of training exclusively on G2P, we additionally create examples for the opposite direction, phoneme-to-grapheme conversion (P2G). We then perform multi-task training on both tasks. (ii) We produce ensembles of our models via majority voting. Our approaches, though being conceptually simple, result in systems that place 6th and 8th amongst 23 submitted systems, and obtain the best results out of all systems on Lithuanian and Modern Greek, respectively.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="singer-kann-2020-nyu" class="col-sm-8">
    
      <div class="title">The NYU-CUBoulder Systems for SIGMORPHON 2020 Task 0 and Task 2</div>
      <div class="author">
        
          
            
              
                
                  Assaf Singer,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe the NYU-CUBoulder systems for the SIGMORPHON 2020 Task 0 on typologically diverse morphological inflection and Task 2 on unsupervised morphological paradigm completion. The former consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. The latter requires generating entire paradigms for a set of given lemmas from raw text alone. We model morphological inflection as a sequence-to-sequence problem, where the input is the sequence of the lemmaâ€™s characters with morphological tags, and the output is the sequence of the inflected formâ€™s characters. First, we apply a transformer model to the task. Second, as inflected forms share most characters with the lemma, we further propose a pointer-generator transformer model to allow easy copying of input characters.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mager-kann-2020-ims" class="col-sm-8">
    
      <div class="title">The IMSâ€“CUBoulder System for the SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion</div>
      <div class="author">
        
          
            
              
                
                  Manuel Mager,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMSâ€“CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mohananey-etal-2020-self" class="col-sm-8">
    
      <div class="title">Self-Training for Unsupervised Parsing with PRPN</div>
      <div class="author">
        
          
            
              
                
                  Anhad Mohananey,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Samuel R. Bowman
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like language modeling. In this work, we propose self-training for neural UP models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. To be able to use our modelâ€™s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with parses predicted by our model to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1% F1 and the previous state of the art by 1.6% F1. In addition, we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="pruksachatkun-etal-2020-intermediate" class="col-sm-8">
    
      <div class="title">Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?</div>
      <div class="author">
        
          
            
              
                
                  Yada Pruksachatkun,
                
              
            
          
        
          
            
              
                
                  Jason Phang,
                
              
            
          
        
          
            
              
                
                  Haokun Liu,
                
              
            
          
        
          
            
              
                
                  Phu Mon Htut,
                
              
            
          
        
          
            
              
                
                  Xiaoyi Zhang,
                
              
            
          
        
          
            
              
                
                  Richard Yuanzhe Pang,
                
              
            
          
        
          
            
              
                
                  Clara Vania,
                
              
            
          
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  and Samuel R. Bowman
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="jin-etal-2020-unsupervised" class="col-sm-8">
    
      <div class="title">Unsupervised Morphological Paradigm Completion</div>
      <div class="author">
        
          
            
              
                
                  Huiming Jin,
                
              
            
          
        
          
            
              
                
                  Liwei Cai,
                
              
            
          
        
          
            
              
                
                  Yihui Peng,
                
              
            
          
        
          
            
              
                
                  Chen Xia,
                
              
            
          
        
          
            
              
                
                  Arya McCarthy,
                
              
            
          
        
          
            
              
                and <em>Katharina Kann</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann2020learning" class="col-sm-8">
    
      <div class="title">Learning to Learn Morphological Inflection for Resource-Poor Languages</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  Samuel R. Bowman,
                
              
            
          
        
          
            
              
                
                  and Kyunghyun Cho
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann2020weakly" class="col-sm-8">
    
      <div class="title">Weakly Supervised POS Taggers Perform Poorly on Truly Low-Resource Languages</div>
      <div class="author">
        
          
            
              
                <em>Katharina Kann</em>,
              
            
          
        
          
            
              
                
                  OphÃ©lie Lacroix,
                
              
            
          
        
          
            
              
                
                  and Anders SÃ¸gaard
                
              
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="kann-2020-acquisition" class="col-sm-8">
    
      <div class="title">Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge</div>
      <div class="author">
        
          
          
              <em>Katharina Kann</em>
            
          
        
      </div>

      <div class="periodical">
      
        In <em>Proceedings of the Society for Computation in Linguistics</em>,
      
      
        2020
      
      </div>
    
    
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 NALA  Group.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: July 22, 2025.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha512-/DXTXr6nQodMUiq+IUJYCt2PPOUjrHJ9wFrqpJ3XkgPNOZVfMok7cRw6CSxyCQxXn6ozlESsSh1/sMCTF1rL/g==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>





<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-88966870-5', 'auto');
ga('send', 'pageview');
</script>



</html>
